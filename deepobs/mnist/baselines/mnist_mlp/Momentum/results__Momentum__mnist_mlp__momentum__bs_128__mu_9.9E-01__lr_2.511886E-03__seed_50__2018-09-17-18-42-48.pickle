(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.14102056622505188
aF0.9174248576164246
aF0.9625197649002075
aF0.9756724834442139
aF0.9830893874168396
aF0.9836827516555786
asVtime/percentage_convergence_performance
p5
(lp6
F0.1448722779750824
aF0.9507943987846375
aF0.9914728403091431
aF1.0001386404037476
aF1.0047264099121094
aF0.9993230700492859
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.14052610099315643
aF0.9222705960273743
aF0.9617286324501038
aF0.9701344966888428
aF0.9745846390724182
aF0.969343364238739
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'nesterov'
p23
I00
sS'train_log_interval'
p24
I10
sS'nologs'
p25
I00
sS'num_epochs'
p26
I5
sS'lr_sched_epochs'
p27
NsS'saveto'
p28
S'res/benchmark_small_final/'
p29
sS'lr_sched_factors'
p30
NsS'mu'
p31
F0.99
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.002511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I50
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sS'run_name'
p41
S'Momentum/'
p42
sbsVtraining/training_loss
p43
(lp44
F2.3020145893096924
aF2.3002309799194336
aF2.2997817993164062
aF2.296351432800293
aF2.291734218597412
aF2.2852349281311035
aF2.2825937271118164
aF2.273996353149414
aF2.248378276824951
aF2.232306480407715
aF2.213233232498169
aF2.1225900650024414
aF2.06510853767395
aF1.963275671005249
aF1.8143901824951172
aF1.526218056678772
aF1.2624014616012573
aF1.0193167924880981
aF0.9924351572990417
aF0.954067587852478
aF0.8923618793487549
aF0.9265908002853394
aF0.49861621856689453
aF0.8570056557655334
aF0.8056086301803589
aF0.5674263834953308
aF0.5914116501808167
aF0.5327286720275879
aF0.7175523638725281
aF0.641864538192749
aF0.7202440500259399
aF0.4566898047924042
aF0.5500784516334534
aF0.6236581802368164
aF0.5801621675491333
aF0.35856443643569946
aF0.41286250948905945
aF0.3091713488101959
aF0.4972946047782898
aF0.36585670709609985
aF0.20516036450862885
aF0.31499236822128296
aF0.2920066714286804
aF0.29691654443740845
aF0.171608567237854
aF0.3815610408782959
aF0.30376166105270386
aF0.4341128468513489
aF0.26012974977493286
aF0.23962250351905823
aF0.27350836992263794
aF0.31061112880706787
aF0.33437907695770264
aF0.1891886591911316
aF0.1585172414779663
aF0.3618306517601013
aF0.22850921750068665
aF0.20119351148605347
aF0.1870761513710022
aF0.1914646029472351
aF0.1411794126033783
aF0.15916049480438232
aF0.22996729612350464
aF0.2005765736103058
aF0.19513122737407684
aF0.1539844423532486
aF0.1747845560312271
aF0.17817717790603638
aF0.14336732029914856
aF0.20383046567440033
aF0.13918408751487732
aF0.14332401752471924
aF0.16758117079734802
aF0.3279486894607544
aF0.1515357941389084
aF0.21290431916713715
aF0.2200985997915268
aF0.11708881705999374
aF0.09779521822929382
aF0.17262494564056396
aF0.12818342447280884
aF0.24234052002429962
aF0.16306422650814056
aF0.1073131114244461
aF0.11580239236354828
aF0.14436553418636322
aF0.11046075820922852
aF0.06408801674842834
aF0.08006369322538376
aF0.09603343904018402
aF0.1482057273387909
aF0.06370246410369873
aF0.16961196064949036
aF0.052241675555706024
aF0.14283359050750732
aF0.12884894013404846
aF0.14231392741203308
aF0.10140789300203323
aF0.14975184202194214
aF0.13990655541419983
aF0.11212316155433655
aF0.1852649450302124
aF0.11296479403972626
aF0.16978102922439575
aF0.09218870103359222
aF0.07814418524503708
aF0.07083888351917267
aF0.03630964457988739
aF0.1402669996023178
aF0.0710991695523262
aF0.16633956134319305
aF0.06920664757490158
aF0.09992408752441406
aF0.15325647592544556
aF0.047466181218624115
aF0.06091829761862755
aF0.06103654205799103
aF0.07319917529821396
aF0.0904289036989212
aF0.07598016411066055
aF0.17058444023132324
aF0.0760464072227478
aF0.04487410560250282
aF0.06896409392356873
aF0.05870121344923973
aF0.10393077880144119
aF0.1649852842092514
aF0.1814245581626892
aF0.12801316380500793
aF0.08057084679603577
aF0.16177675127983093
aF0.1353527009487152
aF0.09541290998458862
aF0.11718524992465973
aF0.12725713849067688
aF0.06708693504333496
aF0.056148722767829895
aF0.06582009792327881
aF0.07720136642456055
aF0.09942933917045593
aF0.06063222512602806
aF0.15658706426620483
aF0.07999085634946823
aF0.07019747793674469
aF0.08091972768306732
aF0.08271236717700958
aF0.058362554758787155
aF0.09572431445121765
aF0.10861510038375854
aF0.06500513851642609
aF0.09989315271377563
aF0.02528315782546997
aF0.05007373169064522
aF0.12819424271583557
aF0.041038237512111664
aF0.10195724666118622
aF0.18468305468559265
aF0.032346196472644806
aF0.047082118690013885
aF0.10730760544538498
aF0.04382767900824547
aF0.06157471612095833
aF0.05231780186295509
aF0.05790025740861893
aF0.04337093606591225
aF0.13946980237960815
aF0.04156012088060379
aF0.07965601235628128
aF0.0910143330693245
aF0.08704563975334167
aF0.03953712806105614
aF0.10592919588088989
aF0.1308780014514923
aF0.06904466450214386
aF0.04498721659183502
aF0.05802362039685249
aF0.1106388047337532
aF0.035683296620845795
aF0.06713319569826126
aF0.08081629127264023
aF0.16239389777183533
aF0.008856559172272682
aF0.03765126317739487
aF0.030637826770544052
aF0.048440832644701004
aF0.08945512771606445
aF0.11007508635520935
aF0.06070267781615257
aF0.03832303732633591
aF0.16120150685310364
aF0.1061115562915802
aF0.061120688915252686
aF0.011832455173134804
aF0.037917204201221466
aF0.018392927944660187
aF0.02457730658352375
aF0.04186869412660599
aF0.0259439367800951
aF0.11545459926128387
aF0.04401039332151413
aF0.07774807512760162
aF0.03499573469161987
aF0.03570453077554703
aF0.026819655671715736
aF0.07742562890052795
aF0.04741213470697403
aF0.04106920585036278
aF0.05763368308544159
aF0.03670118376612663
aF0.06831847131252289
aF0.10578931123018265
aF0.027722805738449097
aF0.04526762664318085
aF0.027229096740484238
aF0.04170353710651398
aF0.021316807717084885
aF0.05171780288219452
aF0.013922519981861115
aF0.11492565274238586
aF0.022893041372299194
aF0.050429485738277435
aF0.03696516528725624
aF0.07490840554237366
aF0.02082430198788643
aF0.04277810454368591
aF0.10257437080144882
aF0.04965149983763695
aF0.06193389743566513
aF0.006293203681707382
aF0.022905543446540833
aF0.06522277742624283
aF0.09747698158025742
aF0.019500287249684334
aF0.011142479255795479
aF0.0694088265299797
asVcheckpoint/checkpoint_train_loss
p45
(lp46
F2.3011279106140137
aF0.28675851225852966
aF0.11936221271753311
aF0.07816571742296219
aF0.053209733217954636
aF0.0518367625772953
asVcheckpoint/checkpoint_test_loss
p47
(lp48
F2.3012073040008545
aF0.2615026831626892
aF0.1199229285120964
aF0.09680642932653427
aF0.08303643763065338
aF0.09418586641550064
asVhyperparams/momentum
p49
(lp50
F0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
asVhyperparams/learning_rate
p51
(lp52
F0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
asVtime/convergence_iterations
p53
(lp54
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.