(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.1217365488409996
aF0.9521360993385315
aF0.978342592716217
aF0.9870451092720032
aF0.9898141026496887
aF0.9921875
asVtime/percentage_convergence_performance
p5
(lp6
F0.11754942685365677
aF0.9790347814559937
aF1.0014640092849731
aF1.0049303770065308
aF1.0062557458877563
aF1.0084986686706543
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.11402294039726257
aF0.9496637582778931
aF0.9714201092720032
aF0.974782407283783
aF0.9760680198669434
aF0.9782436490058899
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'train_log_interval'
p23
I10
sS'nologs'
p24
I00
sS'num_epochs'
p25
I5
sS'lr_sched_epochs'
p26
NsS'saveto'
p27
S'res/benchmark_small_final/'
p28
sS'lr_sched_factors'
p29
NsS'run_name'
p30
S'SGD/'
p31
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.2511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I48
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sbsVtraining/training_loss
p41
(lp42
F2.3020853996276855
aF2.2836625576019287
aF2.226576566696167
aF2.1075944900512695
aF1.825913429260254
aF2.5581154823303223
aF0.9176317453384399
aF1.110237717628479
aF0.6885051727294922
aF0.5550740361213684
aF0.5621626973152161
aF0.6907414197921753
aF0.655229926109314
aF0.5421076416969299
aF0.31016623973846436
aF0.27403509616851807
aF0.28542426228523254
aF0.26152101159095764
aF0.334202378988266
aF0.31660589575767517
aF0.33497679233551025
aF0.2683138847351074
aF0.2760161757469177
aF0.2172010838985443
aF0.2742118835449219
aF0.21897991001605988
aF0.25450125336647034
aF0.34241151809692383
aF0.22290179133415222
aF0.20971286296844482
aF0.1709422767162323
aF0.17383569478988647
aF0.15493157505989075
aF0.30751487612724304
aF0.2563300132751465
aF0.3541029691696167
aF0.18991419672966003
aF0.15263928472995758
aF0.1333780586719513
aF0.2341136932373047
aF0.07670275866985321
aF0.09728744626045227
aF0.2140982747077942
aF0.20020416378974915
aF0.2385004460811615
aF0.09275873005390167
aF0.1185782253742218
aF0.12152852863073349
aF0.3230407238006592
aF0.07341106235980988
aF0.11932031810283661
aF0.14139598608016968
aF0.1512763500213623
aF0.15655626356601715
aF0.2359658181667328
aF0.1627224236726761
aF0.13275308907032013
aF0.08555971086025238
aF0.14379745721817017
aF0.21361631155014038
aF0.1682344675064087
aF0.13340285420417786
aF0.044818900525569916
aF0.15148447453975677
aF0.08385713398456573
aF0.14318731427192688
aF0.08788169175386429
aF0.09993745386600494
aF0.10327759385108948
aF0.11570730805397034
aF0.05601684749126434
aF0.1552322506904602
aF0.12721702456474304
aF0.1384878009557724
aF0.16515177488327026
aF0.08807656168937683
aF0.07225626707077026
aF0.15815818309783936
aF0.06608879566192627
aF0.08061961829662323
aF0.14210709929466248
aF0.057591620832681656
aF0.08083012700080872
aF0.063299760222435
aF0.08011413365602493
aF0.06288962066173553
aF0.0566597580909729
aF0.06340088695287704
aF0.03269745409488678
aF0.11195632815361023
aF0.08693899214267731
aF0.08060882985591888
aF0.06051002815365791
aF0.1255762279033661
aF0.128534197807312
aF0.16013574600219727
aF0.1275370717048645
aF0.031205574050545692
aF0.08661217242479324
aF0.060251038521528244
aF0.07732366025447845
aF0.058590278029441833
aF0.15049470961093903
aF0.12274695932865143
aF0.12807950377464294
aF0.08722579479217529
aF0.07503349334001541
aF0.026224572211503983
aF0.0935339480638504
aF0.04888453334569931
aF0.06293728947639465
aF0.08811023086309433
aF0.04687102511525154
aF0.0671519860625267
aF0.05157720297574997
aF0.07327517122030258
aF0.04767505079507828
aF0.0635489672422409
aF0.07866782695055008
aF0.10639193654060364
aF0.045836567878723145
aF0.030759554356336594
aF0.04076603427529335
aF0.05147961899638176
aF0.1880737692117691
aF0.028276704251766205
aF0.1927204132080078
aF0.08911170065402985
aF0.06773264706134796
aF0.1215357631444931
aF0.03352297097444534
aF0.06755032390356064
aF0.08659682422876358
aF0.047714993357658386
aF0.1018504798412323
aF0.057228196412324905
aF0.05733742192387581
aF0.06388451159000397
aF0.01756049506366253
aF0.050801269710063934
aF0.030190443620085716
aF0.04476260021328926
aF0.06301909685134888
aF0.019809862598776817
aF0.035059258341789246
aF0.05165285989642143
aF0.11557088792324066
aF0.029937494546175003
aF0.06612388789653778
aF0.057901475578546524
aF0.11517838388681412
aF0.09272223711013794
aF0.06264308094978333
aF0.053995974361896515
aF0.0711590051651001
aF0.0680127888917923
aF0.019203413277864456
aF0.07027541100978851
aF0.10691171139478683
aF0.01306287944316864
aF0.03287671133875847
aF0.17089708149433136
aF0.02784862369298935
aF0.04285227134823799
aF0.06469419598579407
aF0.05226185917854309
aF0.060482457280159
aF0.029992209747433662
aF0.06197259575128555
aF0.044557198882102966
aF0.028997620567679405
aF0.05079851672053337
aF0.04853495955467224
aF0.07970468699932098
aF0.05389091372489929
aF0.0804058089852333
aF0.07359007745981216
aF0.028135117143392563
aF0.050597596913576126
aF0.020028987899422646
aF0.016212444752454758
aF0.04551782086491585
aF0.06354077160358429
aF0.02617405727505684
aF0.010837169364094734
aF0.05909453332424164
aF0.050824426114559174
aF0.09325360506772995
aF0.08597933501005173
aF0.10975299030542374
aF0.035453297197818756
aF0.04819031059741974
aF0.03944754600524902
aF0.01939144916832447
aF0.02742902748286724
aF0.049989569932222366
aF0.05863295495510101
aF0.024356020614504814
aF0.037004388868808746
aF0.018820129334926605
aF0.005172042176127434
aF0.02053147926926613
aF0.01125409733504057
aF0.03501033037900925
aF0.0605793334543705
aF0.06117381155490875
aF0.0649893507361412
aF0.03745824843645096
aF0.023055020719766617
aF0.027532659471035004
aF0.051520418375730515
aF0.09065031260251999
aF0.020010823383927345
aF0.007017694413661957
aF0.009584629908204079
aF0.02118697576224804
aF0.10141991823911667
aF0.06262306869029999
aF0.07302125543355942
aF0.012336557731032372
aF0.05903066694736481
aF0.05003656446933746
aF0.05261094868183136
aF0.1256340593099594
aF0.04573528468608856
aF0.007550169713795185
aF0.017393797636032104
aF0.03486574441194534
aF0.10522497445344925
aF0.023018304258584976
aF0.045569565147161484
aF0.025856517255306244
aF0.03248423710465431
aF0.012077349238097668
aF0.012957212515175343
asVcheckpoint/checkpoint_train_loss
p43
(lp44
F2.3032240867614746
aF0.1559697538614273
aF0.07415060698986053
aF0.04809308052062988
aF0.03780007362365723
aF0.028360525146126747
asVcheckpoint/checkpoint_test_loss
p45
(lp46
F2.303661346435547
aF0.15981121361255646
aF0.09458105266094208
aF0.07966996729373932
aF0.07810833305120468
aF0.07608480006456375
asVhyperparams/learning_rate
p47
(lp48
F0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
asVtime/convergence_iterations
p49
(lp50
F0.0
aF0.0
aF2.0
aF2.0
aF2.0
aF2.0
as.