(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.08702531456947327
aF0.912875771522522
aF0.9662777185440063
aF0.9798259735107422
aF0.9829905033111572
aF0.9872428774833679
asVtime/percentage_convergence_performance
p5
(lp6
F0.0926733985543251
aF0.9445753693580627
aF0.9905552864074707
aF1.0018718242645264
aF1.0066635608673096
aF1.006051778793335
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.08989319950342178
aF0.9162381291389465
aF0.9608386158943176
aF0.9718156456947327
aF0.9764636158943176
aF0.9758702516555786
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'nesterov'
p23
I00
sS'train_log_interval'
p24
I10
sS'nologs'
p25
I00
sS'num_epochs'
p26
I5
sS'lr_sched_epochs'
p27
NsS'saveto'
p28
S'res/benchmark_small_final/'
p29
sS'lr_sched_factors'
p30
NsS'mu'
p31
F0.99
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.002511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I42
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sS'run_name'
p41
S'Momentum/'
p42
sbsVtraining/training_loss
p43
(lp44
F2.303393602371216
aF2.3022522926330566
aF2.3009233474731445
aF2.296022415161133
aF2.2888026237487793
aF2.290611743927002
aF2.2786643505096436
aF2.2716143131256104
aF2.2616381645202637
aF2.247074604034424
aF2.227079153060913
aF2.1848864555358887
aF2.163252353668213
aF2.011625289916992
aF1.8997318744659424
aF1.709666132926941
aF1.4110653400421143
aF1.1764941215515137
aF0.9885456562042236
aF0.8755856156349182
aF0.8436201810836792
aF0.7005748748779297
aF0.8903540372848511
aF0.41147011518478394
aF0.9713541865348816
aF0.841933012008667
aF0.8274015188217163
aF0.7757009267807007
aF0.649089515209198
aF0.43868160247802734
aF0.6455502510070801
aF0.5279049873352051
aF0.8395448923110962
aF0.334725946187973
aF0.5075469613075256
aF0.3737751841545105
aF0.3889365494251251
aF0.48635029792785645
aF0.3608207106590271
aF0.3398447632789612
aF0.3234866261482239
aF0.2942383885383606
aF0.2712985873222351
aF0.4016638994216919
aF0.16469129920005798
aF0.2250918745994568
aF0.16282597184181213
aF0.20283114910125732
aF0.2847992181777954
aF0.4160391092300415
aF0.2553901672363281
aF0.27199864387512207
aF0.32703304290771484
aF0.28016412258148193
aF0.28813573718070984
aF0.2595880627632141
aF0.3322610855102539
aF0.16833020746707916
aF0.3735096752643585
aF0.21667508780956268
aF0.17287996411323547
aF0.2064041793346405
aF0.1469728648662567
aF0.35743075609207153
aF0.31225472688674927
aF0.17675210535526276
aF0.3617827594280243
aF0.16346193850040436
aF0.2216632068157196
aF0.09468042850494385
aF0.15725748240947723
aF0.12913727760314941
aF0.13263435661792755
aF0.27338072657585144
aF0.16158610582351685
aF0.14259353280067444
aF0.12684211134910583
aF0.223744735121727
aF0.11467545479536057
aF0.14049836993217468
aF0.4694674611091614
aF0.1391448825597763
aF0.19108812510967255
aF0.13098126649856567
aF0.11251527816057205
aF0.09330824017524719
aF0.14389945566654205
aF0.06621457636356354
aF0.15257063508033752
aF0.05133545398712158
aF0.10208719968795776
aF0.11565317213535309
aF0.039953216910362244
aF0.24048590660095215
aF0.1482541859149933
aF0.0911390632390976
aF0.16782990097999573
aF0.17226165533065796
aF0.07282660901546478
aF0.10398688167333603
aF0.11035522818565369
aF0.11300969123840332
aF0.0853334590792656
aF0.07790884375572205
aF0.07965697348117828
aF0.06560365110635757
aF0.07903165370225906
aF0.0990273654460907
aF0.12290705740451813
aF0.27037060260772705
aF0.0853116512298584
aF0.19249242544174194
aF0.12545709311962128
aF0.05225484073162079
aF0.09542424976825714
aF0.12338753044605255
aF0.09806957095861435
aF0.15731845796108246
aF0.09887486696243286
aF0.13154402375221252
aF0.08805321902036667
aF0.07772524654865265
aF0.23754113912582397
aF0.09697121381759644
aF0.1371196210384369
aF0.13491524755954742
aF0.04691891744732857
aF0.06472434103488922
aF0.04924673214554787
aF0.20861534774303436
aF0.07435256987810135
aF0.20656436681747437
aF0.17758911848068237
aF0.1280270367860794
aF0.18883544206619263
aF0.11793652176856995
aF0.05224022641777992
aF0.17597030103206635
aF0.1230560690164566
aF0.13043421506881714
aF0.02523045241832733
aF0.0659848004579544
aF0.12448814511299133
aF0.09651198238134384
aF0.0152966920286417
aF0.08362841606140137
aF0.15659792721271515
aF0.04289130121469498
aF0.0524572916328907
aF0.08739820867776871
aF0.14722155034542084
aF0.08140752464532852
aF0.15254417061805725
aF0.06724335998296738
aF0.1174979954957962
aF0.07688385993242264
aF0.08898968249559402
aF0.06345924735069275
aF0.07537080347537994
aF0.08740891516208649
aF0.08229411393404007
aF0.10916048288345337
aF0.06809672713279724
aF0.04433606192469597
aF0.05258568748831749
aF0.06122859939932823
aF0.01874144747853279
aF0.17667743563652039
aF0.0960477963089943
aF0.0697169080376625
aF0.04147284850478172
aF0.0545487180352211
aF0.06451296806335449
aF0.03185031935572624
aF0.035891689360141754
aF0.0297478549182415
aF0.03717637062072754
aF0.1239890307188034
aF0.037840764969587326
aF0.011553523130714893
aF0.09970428049564362
aF0.06844823807477951
aF0.018264267593622208
aF0.015678629279136658
aF0.10109133273363113
aF0.03467652574181557
aF0.014382537454366684
aF0.03490413725376129
aF0.03975670784711838
aF0.02826596423983574
aF0.02729007601737976
aF0.06090528890490532
aF0.018558844923973083
aF0.13642284274101257
aF0.11155552417039871
aF0.07015746831893921
aF0.04798142984509468
aF0.03708849847316742
aF0.019127093255519867
aF0.021352585405111313
aF0.009381402283906937
aF0.051586173474788666
aF0.04426315799355507
aF0.03353765234351158
aF0.054995737969875336
aF0.04694107547402382
aF0.049696072936058044
aF0.014322564005851746
aF0.045923247933387756
aF0.09516459703445435
aF0.037380412220954895
aF0.010814212262630463
aF0.09025411307811737
aF0.07238951325416565
aF0.045893020927906036
aF0.010172129608690739
aF0.03704352304339409
aF0.03791781887412071
aF0.08892115950584412
aF0.04055991768836975
aF0.032172076404094696
aF0.13419808447360992
aF0.019003983587026596
aF0.048802610486745834
aF0.02834853157401085
aF0.024933043867349625
aF0.1118062362074852
aF0.054647862911224365
aF0.03166372328996658
aF0.049251262098550797
aF0.05311379209160805
aF0.06156536191701889
aF0.015044664032757282
aF0.047475170344114304
aF0.024297673255205154
asVcheckpoint/checkpoint_train_loss
p45
(lp46
F2.3027536869049072
aF0.28992703557014465
aF0.11346308887004852
aF0.06825720518827438
aF0.05304982140660286
aF0.03890244662761688
asVcheckpoint/checkpoint_test_loss
p47
(lp48
F2.3027360439300537
aF0.27919119596481323
aF0.12691494822502136
aF0.08927620947360992
aF0.07900727540254593
aF0.0772799625992775
asVhyperparams/momentum
p49
(lp50
F0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
asVhyperparams/learning_rate
p51
(lp52
F0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
asVtime/convergence_iterations
p53
(lp54
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.