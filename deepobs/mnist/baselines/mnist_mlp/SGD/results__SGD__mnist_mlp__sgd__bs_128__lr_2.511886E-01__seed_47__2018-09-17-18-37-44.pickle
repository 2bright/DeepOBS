(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.07525712251663208
aF0.9553006291389465
aF0.9775514006614685
aF0.9854628443717957
aF0.989022970199585
aF0.9923852682113647
asVtime/percentage_convergence_performance
p5
(lp6
F0.08461927622556686
aF0.9841323494911194
aF1.000342607498169
aF1.0052361488342285
aF1.006561517715454
aF1.008090853691101
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.08208069950342178
aF0.9546083807945251
aF0.9703322649002075
aF0.9750791192054749
aF0.9763647317886353
aF0.9778481125831604
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'train_log_interval'
p23
I10
sS'nologs'
p24
I00
sS'num_epochs'
p25
I5
sS'lr_sched_epochs'
p26
NsS'saveto'
p27
S'res/benchmark_small_final/'
p28
sS'lr_sched_factors'
p29
NsS'run_name'
p30
S'SGD/'
p31
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.2511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I47
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sbsVtraining/training_loss
p41
(lp42
F2.3041162490844727
aF2.286707878112793
aF2.2643182277679443
aF2.1541624069213867
aF1.6844050884246826
aF1.6803205013275146
aF1.214475393295288
aF1.6447346210479736
aF0.7410317659378052
aF1.3719065189361572
aF0.5130607485771179
aF0.8231948614120483
aF0.4507977068424225
aF0.488544225692749
aF0.4028237462043762
aF0.40118491649627686
aF0.3275684714317322
aF0.5299426913261414
aF0.43496379256248474
aF0.25744500756263733
aF0.4186497926712036
aF0.25412923097610474
aF0.3394659757614136
aF0.24591657519340515
aF0.318069726228714
aF0.2647777199745178
aF0.20986352860927582
aF0.24197903275489807
aF0.1770770251750946
aF0.23265984654426575
aF0.2839187979698181
aF0.2658076882362366
aF0.26075106859207153
aF0.22214382886886597
aF0.29313749074935913
aF0.24613720178604126
aF0.2232692539691925
aF0.1248636543750763
aF0.28788530826568604
aF0.2001412808895111
aF0.2052265703678131
aF0.1721613109111786
aF0.2573689818382263
aF0.25708404183387756
aF0.30031120777130127
aF0.1159851998090744
aF0.07703272253274918
aF0.12914419174194336
aF0.2270544171333313
aF0.1355265974998474
aF0.10776828229427338
aF0.12454967200756073
aF0.18397191166877747
aF0.28576016426086426
aF0.10228555649518967
aF0.124336376786232
aF0.10688471049070358
aF0.06311411410570145
aF0.12444519251585007
aF0.15276466310024261
aF0.17266036570072174
aF0.20431993901729584
aF0.24938663840293884
aF0.13336440920829773
aF0.16061685979366302
aF0.2821897268295288
aF0.12237057089805603
aF0.130830317735672
aF0.08654148131608963
aF0.12995904684066772
aF0.06678339838981628
aF0.10823938995599747
aF0.12906073033809662
aF0.12665262818336487
aF0.22779443860054016
aF0.14710792899131775
aF0.24303506314754486
aF0.18266209959983826
aF0.14148205518722534
aF0.14036133885383606
aF0.1069418415427208
aF0.1126769557595253
aF0.07920205593109131
aF0.22984586656093597
aF0.033058010041713715
aF0.07484807074069977
aF0.056290969252586365
aF0.08005164563655853
aF0.0969301164150238
aF0.07862840592861176
aF0.0910511240363121
aF0.12877987325191498
aF0.08546970784664154
aF0.10279606282711029
aF0.07251784950494766
aF0.13072018325328827
aF0.03124883398413658
aF0.1461002081632614
aF0.071402408182621
aF0.0247313492000103
aF0.09192202240228653
aF0.06711055338382721
aF0.029437284916639328
aF0.15150997042655945
aF0.06784287095069885
aF0.08377134799957275
aF0.07952874898910522
aF0.04777402803301811
aF0.05301729589700699
aF0.1217353343963623
aF0.04348155856132507
aF0.05096331238746643
aF0.07072533667087555
aF0.12410500645637512
aF0.11127548664808273
aF0.03562320023775101
aF0.05500151962041855
aF0.07133035361766815
aF0.04936809837818146
aF0.07925642281770706
aF0.1970941126346588
aF0.06581951677799225
aF0.0670989453792572
aF0.06770849972963333
aF0.15969035029411316
aF0.04065528139472008
aF0.06694159656763077
aF0.057311199605464935
aF0.09486989676952362
aF0.05878972262144089
aF0.055031321942806244
aF0.07907150685787201
aF0.0952778235077858
aF0.11228429526090622
aF0.07819319516420364
aF0.11646615713834763
aF0.08853678405284882
aF0.06778381764888763
aF0.07059597969055176
aF0.0360688790678978
aF0.0523676760494709
aF0.07042066752910614
aF0.0672476664185524
aF0.045105110853910446
aF0.11961449682712555
aF0.011671189218759537
aF0.01683032140135765
aF0.04169340804219246
aF0.022679155692458153
aF0.09715401381254196
aF0.09777853637933731
aF0.10191085189580917
aF0.014500061981379986
aF0.021440990269184113
aF0.02530771866440773
aF0.02427300252020359
aF0.11259614676237106
aF0.014401200227439404
aF0.028376473113894463
aF0.040428563952445984
aF0.03237579017877579
aF0.13878363370895386
aF0.010214870795607567
aF0.02710450440645218
aF0.07585634291172028
aF0.06401626765727997
aF0.05251370370388031
aF0.08057456463575363
aF0.06705982983112335
aF0.05085155740380287
aF0.04452085494995117
aF0.017894456163048744
aF0.05155109986662865
aF0.09270134568214417
aF0.05016208067536354
aF0.01834157109260559
aF0.09703284502029419
aF0.16293838620185852
aF0.06719285249710083
aF0.04141493886709213
aF0.0399644635617733
aF0.03333427011966705
aF0.030947236344218254
aF0.1059107631444931
aF0.06827475130558014
aF0.02109041064977646
aF0.03612323850393295
aF0.03097650781273842
aF0.17945727705955505
aF0.01349775493144989
aF0.010074462741613388
aF0.07688800990581512
aF0.04963049665093422
aF0.021811529994010925
aF0.0322660394012928
aF0.060703396797180176
aF0.112408347427845
aF0.0234929621219635
aF0.011503292247653008
aF0.027532469481229782
aF0.05607638135552406
aF0.04052167385816574
aF0.02618549019098282
aF0.03318435326218605
aF0.0674855187535286
aF0.024355681613087654
aF0.020108187571167946
aF0.045089006423950195
aF0.08755616843700409
aF0.03507356345653534
aF0.021804070100188255
aF0.05686074122786522
aF0.0783100351691246
aF0.05539678782224655
aF0.049042873084545135
aF0.01358394231647253
aF0.04428023844957352
aF0.02787915989756584
aF0.02800900861620903
aF0.06362995505332947
aF0.010471846908330917
aF0.043242357671260834
aF0.04695209860801697
aF0.011251281946897507
aF0.038666173815727234
aF0.06564986705780029
aF0.01582135632634163
aF0.014993595890700817
aF0.004573676269501448
aF0.012296512722969055
aF0.035246122628450394
aF0.01758228987455368
aF0.016552066430449486
aF0.009868092834949493
aF0.05231240391731262
asVcheckpoint/checkpoint_train_loss
p43
(lp44
F2.3041746616363525
aF0.14955845475196838
aF0.07900477945804596
aF0.05075258016586304
aF0.03663279488682747
aF0.026444297283887863
asVcheckpoint/checkpoint_test_loss
p45
(lp46
F2.304405450820923
aF0.1471565067768097
aF0.09508947283029556
aF0.07846013456583023
aF0.07258282601833344
aF0.07071670144796371
asVhyperparams/learning_rate
p47
(lp48
F0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
asVtime/convergence_iterations
p49
(lp50
F0.0
aF0.0
aF2.0
aF2.0
aF2.0
aF2.0
as.