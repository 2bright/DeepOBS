(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.12480221688747406
aF0.962717592716217
aF0.9775514006614685
aF0.9838805198669434
aF0.9878362417221069
aF0.986155092716217
asVtime/percentage_convergence_performance
p5
(lp6
F0.13121084868907928
aF0.9840304255485535
aF0.9989152550697327
aF1.0047264099121094
aF1.007581114768982
aF1.0063576698303223
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.1272745281457901
aF0.9545094966888428
aF0.9689477682113647
aF0.9745846390724182
aF0.9773536324501038
aF0.9761669039726257
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'train_log_interval'
p23
I10
sS'nologs'
p24
I00
sS'num_epochs'
p25
I5
sS'lr_sched_epochs'
p26
NsS'saveto'
p27
S'res/benchmark_small_final/'
p28
sS'lr_sched_factors'
p29
NsS'run_name'
p30
S'SGD/'
p31
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.2511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I46
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sbsVtraining/training_loss
p41
(lp42
F2.3016233444213867
aF2.285217761993408
aF2.229979991912842
aF2.0691206455230713
aF1.6392288208007812
aF1.3265492916107178
aF0.9457460641860962
aF1.0543774366378784
aF0.5908410549163818
aF0.622222363948822
aF0.3898516297340393
aF0.8528405427932739
aF0.6531066298484802
aF0.5091590881347656
aF0.36638540029525757
aF0.36235666275024414
aF0.36303508281707764
aF0.3074457347393036
aF0.3133808374404907
aF0.3817455768585205
aF0.24139106273651123
aF0.4069388508796692
aF0.2837410569190979
aF0.33985477685928345
aF0.17399737238883972
aF0.2544805407524109
aF0.40103185176849365
aF0.25589847564697266
aF0.20839089155197144
aF0.2176784873008728
aF0.2227349579334259
aF0.19003257155418396
aF0.2548031806945801
aF0.19737568497657776
aF0.2111356258392334
aF0.11712858080863953
aF0.31048882007598877
aF0.24629250168800354
aF0.11741151660680771
aF0.29721981287002563
aF0.13385377824306488
aF0.21513637900352478
aF0.2223341017961502
aF0.19173195958137512
aF0.1858351081609726
aF0.17273902893066406
aF0.2073645442724228
aF0.31418532133102417
aF0.19583791494369507
aF0.1847560703754425
aF0.17518559098243713
aF0.17303988337516785
aF0.14044474065303802
aF0.0639752447605133
aF0.20336732268333435
aF0.1979074329137802
aF0.15537694096565247
aF0.19307546317577362
aF0.22281993925571442
aF0.07670854777097702
aF0.1810668557882309
aF0.13552692532539368
aF0.19317646324634552
aF0.08937633782625198
aF0.17130962014198303
aF0.10929955542087555
aF0.1365572065114975
aF0.18345054984092712
aF0.2252722680568695
aF0.12674453854560852
aF0.23881587386131287
aF0.0589434951543808
aF0.12053649872541428
aF0.09357296675443649
aF0.19907090067863464
aF0.06858895719051361
aF0.11802732944488525
aF0.08035089075565338
aF0.16010694205760956
aF0.13827405869960785
aF0.20842647552490234
aF0.11706982553005219
aF0.07508180290460587
aF0.04516217112541199
aF0.06685762107372284
aF0.1465388387441635
aF0.0764281302690506
aF0.13232338428497314
aF0.04001409560441971
aF0.05104454606771469
aF0.10285800695419312
aF0.2091740369796753
aF0.15611103177070618
aF0.0385921448469162
aF0.09722612798213959
aF0.06034892052412033
aF0.1511656939983368
aF0.18130697309970856
aF0.11927823722362518
aF0.18738365173339844
aF0.05316590517759323
aF0.03162434697151184
aF0.027726691216230392
aF0.09958536177873611
aF0.05289717763662338
aF0.10962310433387756
aF0.11096115410327911
aF0.14232206344604492
aF0.07084779441356659
aF0.07375703006982803
aF0.11113271117210388
aF0.10668150335550308
aF0.10009069740772247
aF0.045132704079151154
aF0.038793355226516724
aF0.06421917676925659
aF0.11242762953042984
aF0.048731233924627304
aF0.04563267529010773
aF0.10977620631456375
aF0.05044560879468918
aF0.10445582866668701
aF0.05653993785381317
aF0.06952089071273804
aF0.06408736109733582
aF0.20079106092453003
aF0.051688142120838165
aF0.0328327901661396
aF0.033003270626068115
aF0.0902613028883934
aF0.07735230028629303
aF0.07880549132823944
aF0.05066181719303131
aF0.019416868686676025
aF0.030794043093919754
aF0.13376612961292267
aF0.05820097029209137
aF0.0527152381837368
aF0.03631822019815445
aF0.083335742354393
aF0.030390143394470215
aF0.07362693548202515
aF0.04692218452692032
aF0.05444154888391495
aF0.040207743644714355
aF0.02332512103021145
aF0.05212441086769104
aF0.033599112182855606
aF0.03883776068687439
aF0.019254038110375404
aF0.07497172802686691
aF0.029130037873983383
aF0.05167936533689499
aF0.02823253720998764
aF0.052079521119594574
aF0.027616631239652634
aF0.06607358157634735
aF0.028006769716739655
aF0.02785884402692318
aF0.08520321547985077
aF0.06587166339159012
aF0.029359640553593636
aF0.04041421785950661
aF0.03844314068555832
aF0.03804522007703781
aF0.03686805069446564
aF0.03575584292411804
aF0.05297015979886055
aF0.0454561710357666
aF0.06182076036930084
aF0.04345244541764259
aF0.017292441800236702
aF0.08371980488300323
aF0.008950939401984215
aF0.01785203069448471
aF0.013798903673887253
aF0.045537516474723816
aF0.07436298578977585
aF0.05382263660430908
aF0.0658649131655693
aF0.016890987753868103
aF0.018390847370028496
aF0.021770751103758812
aF0.07529705762863159
aF0.03358191251754761
aF0.1373009830713272
aF0.02383427694439888
aF0.03935519605875015
aF0.09369509667158127
aF0.049006253480911255
aF0.0906258225440979
aF0.07047572731971741
aF0.043208710849285126
aF0.027864139527082443
aF0.022520989179611206
aF0.020796461030840874
aF0.049102067947387695
aF0.01812678948044777
aF0.004357314668595791
aF0.030603861436247826
aF0.013498544692993164
aF0.055966686457395554
aF0.10933264344930649
aF0.05016706883907318
aF0.018675919622182846
aF0.005208732094615698
aF0.025449015200138092
aF0.07594206184148788
aF0.05330519378185272
aF0.021873019635677338
aF0.08714969456195831
aF0.014705474488437176
aF0.04006470367312431
aF0.01615213416516781
aF0.04570390284061432
aF0.014746970497071743
aF0.06618355214595795
aF0.005965507589280605
aF0.013227459043264389
aF0.024503976106643677
aF0.023367496207356453
aF0.022199101746082306
aF0.004498094320297241
aF0.03480568528175354
aF0.02592950314283371
aF0.02908291295170784
aF0.01345551572740078
aF0.3684329092502594
aF0.06073657423257828
aF0.08677633851766586
aF0.06616182625293732
aF0.06978000700473785
aF0.010654732584953308
aF0.03747360408306122
aF0.034979384392499924
asVcheckpoint/checkpoint_train_loss
p43
(lp44
F2.3016698360443115
aF0.13407635688781738
aF0.07604530453681946
aF0.0512939989566803
aF0.038297075778245926
aF0.04025965556502342
asVcheckpoint/checkpoint_test_loss
p45
(lp46
F2.3015315532684326
aF0.14742854237556458
aF0.10042525827884674
aF0.08415216952562332
aF0.07844355702400208
aF0.08094187825918198
asVhyperparams/learning_rate
p47
(lp48
F0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
asVtime/convergence_iterations
p49
(lp50
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.