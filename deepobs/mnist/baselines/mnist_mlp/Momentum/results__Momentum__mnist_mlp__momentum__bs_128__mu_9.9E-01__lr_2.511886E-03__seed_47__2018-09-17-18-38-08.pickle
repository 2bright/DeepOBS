(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.07525712251663208
aF0.9157436490058899
aF0.9674643874168396
aF0.9792326092720032
aF0.9779469966888428
aF0.9864517450332642
asVtime/percentage_convergence_performance
p5
(lp6
F0.08461927622556686
aF0.9477358460426331
aF0.9917786717414856
aF1.0001386404037476
aF0.9979977011680603
aF1.005338191986084
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.08208069950342178
aF0.9193037748336792
aF0.9620253443717957
aF0.9701344966888428
aF0.9680577516555786
aF0.9751780033111572
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'nesterov'
p23
I00
sS'train_log_interval'
p24
I10
sS'nologs'
p25
I00
sS'num_epochs'
p26
I5
sS'lr_sched_epochs'
p27
NsS'saveto'
p28
S'res/benchmark_small_final/'
p29
sS'lr_sched_factors'
p30
NsS'mu'
p31
F0.99
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.002511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I47
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sS'run_name'
p41
S'Momentum/'
p42
sbsVtraining/training_loss
p43
(lp44
F2.3041162490844727
aF2.3041563034057617
aF2.3030004501342773
aF2.3004422187805176
aF2.2936906814575195
aF2.290351629257202
aF2.2801263332366943
aF2.2705836296081543
aF2.2578225135803223
aF2.2422733306884766
aF2.2271220684051514
aF2.193510055541992
aF2.149359941482544
aF2.0516111850738525
aF1.9298324584960938
aF1.7126556634902954
aF1.431330680847168
aF1.2005232572555542
aF0.914429783821106
aF0.7891103625297546
aF0.7781698703765869
aF0.8830211162567139
aF0.9062858819961548
aF0.9338744878768921
aF1.0792100429534912
aF0.6009981632232666
aF0.6233582496643066
aF0.7253971099853516
aF0.5911203622817993
aF0.6612250804901123
aF0.6433964371681213
aF0.6692185401916504
aF0.8163068294525146
aF0.6482404470443726
aF0.6321641206741333
aF0.6298128366470337
aF0.40227988362312317
aF0.3738468289375305
aF0.5353884100914001
aF0.42054614424705505
aF0.49711874127388
aF0.4440741539001465
aF0.5219018459320068
aF0.580907940864563
aF0.4240210950374603
aF0.2561948895454407
aF0.19412878155708313
aF0.2673632502555847
aF0.384519100189209
aF0.3510104715824127
aF0.26075929403305054
aF0.22758010029792786
aF0.280269980430603
aF0.4003202021121979
aF0.22757059335708618
aF0.19161531329154968
aF0.20828092098236084
aF0.13930922746658325
aF0.16541728377342224
aF0.21661721169948578
aF0.2714161276817322
aF0.23770655691623688
aF0.3612305521965027
aF0.22141003608703613
aF0.25150036811828613
aF0.4698917269706726
aF0.1929970383644104
aF0.1985766440629959
aF0.11634771525859833
aF0.19472792744636536
aF0.1197146624326706
aF0.18142831325531006
aF0.1674807071685791
aF0.14604495465755463
aF0.31519901752471924
aF0.20686942338943481
aF0.3247072696685791
aF0.2711404263973236
aF0.15110281109809875
aF0.1575486660003662
aF0.12827259302139282
aF0.13362465798854828
aF0.09508586674928665
aF0.2224283516407013
aF0.06003779172897339
aF0.10274484753608704
aF0.08299456536769867
aF0.11928559839725494
aF0.14799939095973969
aF0.09779311716556549
aF0.12396634370088577
aF0.11798586696386337
aF0.10165514796972275
aF0.12852805852890015
aF0.0910160094499588
aF0.15379011631011963
aF0.055719103664159775
aF0.19579565525054932
aF0.07397966086864471
aF0.040249310433864594
aF0.03722885623574257
aF0.06470194458961487
aF0.05326862633228302
aF0.14137637615203857
aF0.0816391259431839
aF0.11348369717597961
aF0.13800688087940216
aF0.07854820787906647
aF0.09429531544446945
aF0.1808575689792633
aF0.09490589052438736
aF0.06508199870586395
aF0.08127790689468384
aF0.17132136225700378
aF0.16161248087882996
aF0.05409888178110123
aF0.10979488492012024
aF0.08613111823797226
aF0.09057366847991943
aF0.11755497753620148
aF0.17251044511795044
aF0.06691603362560272
aF0.11404692381620407
aF0.070931576192379
aF0.11921011656522751
aF0.08329367637634277
aF0.09252506494522095
aF0.1605009138584137
aF0.09479211270809174
aF0.07181090116500854
aF0.0755670815706253
aF0.05931473523378372
aF0.10815734416246414
aF0.1540704369544983
aF0.0947972983121872
aF0.13329057395458221
aF0.0555800162255764
aF0.09505093097686768
aF0.07681635022163391
aF0.056902408599853516
aF0.08204697072505951
aF0.06870768219232559
aF0.07918441295623779
aF0.04841724783182144
aF0.14520013332366943
aF0.011469069868326187
aF0.04007537662982941
aF0.05130365490913391
aF0.025730997323989868
aF0.11429595947265625
aF0.09181281179189682
aF0.1263384073972702
aF0.024787817150354385
aF0.018798932433128357
aF0.04372219741344452
aF0.05787338316440582
aF0.14174307882785797
aF0.052576277405023575
aF0.04818187654018402
aF0.07971131801605225
aF0.05093684419989586
aF0.15737906098365784
aF0.018882524222135544
aF0.07119446992874146
aF0.09446753561496735
aF0.11473555862903595
aF0.07847574353218079
aF0.11793038249015808
aF0.07714786380529404
aF0.04033004119992256
aF0.05857667699456215
aF0.05949614569544792
aF0.047820135951042175
aF0.0991351529955864
aF0.1094546765089035
aF0.032092321664094925
aF0.12130717188119888
aF0.20842179656028748
aF0.07719294726848602
aF0.07982586324214935
aF0.07968702167272568
aF0.044466447085142136
aF0.048769980669021606
aF0.14672133326530457
aF0.06403840333223343
aF0.05083000659942627
aF0.030392128974199295
aF0.07405711710453033
aF0.1492217779159546
aF0.046221837401390076
aF0.020924629643559456
aF0.04970978945493698
aF0.052887506783008575
aF0.022575173527002335
aF0.053753890097141266
aF0.13065530359745026
aF0.03985804319381714
aF0.020501334220170975
aF0.03346835821866989
aF0.06323118507862091
aF0.12812066078186035
aF0.0602881982922554
aF0.06547477096319199
aF0.053181540220975876
aF0.10795827955007553
aF0.027092760428786278
aF0.04603732377290726
aF0.07731914520263672
aF0.11532449722290039
aF0.037486955523490906
aF0.040315281599760056
aF0.027724487707018852
aF0.1414450854063034
aF0.05370146036148071
aF0.01810164749622345
aF0.03407496586441994
aF0.046567581593990326
aF0.02096986398100853
aF0.03868936002254486
aF0.11040002852678299
aF0.04218864440917969
aF0.08312712609767914
aF0.054969944059848785
aF0.0653621256351471
aF0.04579399153590202
aF0.0719192773103714
aF0.017580948770046234
aF0.021855492144823074
aF0.01723368652164936
aF0.008438455872237682
aF0.06304372102022171
aF0.030476579442620277
aF0.019908808171749115
aF0.02533525787293911
aF0.0975370705127716
asVcheckpoint/checkpoint_train_loss
p45
(lp46
F2.3041746616363525
aF0.2917662262916565
aF0.10953619331121445
aF0.06991105526685715
aF0.07068821787834167
aF0.040191296488046646
asVcheckpoint/checkpoint_test_loss
p47
(lp48
F2.304405450820923
aF0.27191025018692017
aF0.12052199244499207
aF0.09389219433069229
aF0.10132206231355667
aF0.07989518344402313
asVhyperparams/momentum
p49
(lp50
F0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
aF0.9900000095367432
asVhyperparams/learning_rate
p51
(lp52
F0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
aF0.0025118859484791756
asVtime/convergence_iterations
p53
(lp54
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.