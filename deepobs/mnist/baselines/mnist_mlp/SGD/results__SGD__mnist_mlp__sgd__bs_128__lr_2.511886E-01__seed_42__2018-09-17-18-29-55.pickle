(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.08702531456947327
aF0.953125
aF0.9741891026496887
aF0.9833860993385315
aF0.9882317781448364
aF0.9892207384109497
asVtime/percentage_convergence_performance
p5
(lp6
F0.0926733985543251
aF0.9773016571998596
aF0.9993230700492859
aF1.0064595937728882
aF1.0091103315353394
aF1.0084986686706543
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.08989319950342178
aF0.9479826092720032
aF0.969343364238739
aF0.9762658476829529
aF0.9788370132446289
aF0.9782436490058899
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'train_log_interval'
p23
I10
sS'nologs'
p24
I00
sS'num_epochs'
p25
I5
sS'lr_sched_epochs'
p26
NsS'saveto'
p27
S'res/benchmark_small_final/'
p28
sS'lr_sched_factors'
p29
NsS'run_name'
p30
S'SGD/'
p31
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.2511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I42
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sbsVtraining/training_loss
p41
(lp42
F2.303393602371216
aF2.288209915161133
aF2.2618935108184814
aF2.149754047393799
aF1.6380890607833862
aF1.3453609943389893
aF1.0172737836837769
aF1.250046730041504
aF0.7503172755241394
aF0.7512052059173584
aF0.7436401844024658
aF0.5843804478645325
aF0.5130367279052734
aF0.397121787071228
aF0.6033368110656738
aF0.388228178024292
aF0.32729727029800415
aF0.383633017539978
aF0.4333885610103607
aF0.3386780917644501
aF0.3434485197067261
aF0.20791298151016235
aF0.36511731147766113
aF0.19669416546821594
aF0.3311295509338379
aF0.20027394592761993
aF0.2307194173336029
aF0.28583401441574097
aF0.26281240582466125
aF0.11037464439868927
aF0.2947952449321747
aF0.29495298862457275
aF0.33531540632247925
aF0.14853082597255707
aF0.16337579488754272
aF0.11650757491588593
aF0.14677606523036957
aF0.14512649178504944
aF0.11379913985729218
aF0.1720072478055954
aF0.14750903844833374
aF0.17137812077999115
aF0.13741208612918854
aF0.21493546664714813
aF0.11667892336845398
aF0.08842440694570541
aF0.16007466614246368
aF0.1399756819009781
aF0.17479202151298523
aF0.35229015350341797
aF0.16394942998886108
aF0.14918747544288635
aF0.1321793794631958
aF0.15884929895401
aF0.1854853481054306
aF0.12466371059417725
aF0.1809127926826477
aF0.11042741686105728
aF0.2285015732049942
aF0.10523752868175507
aF0.060834549367427826
aF0.12369050830602646
aF0.08775389194488525
aF0.2105325162410736
aF0.22039499878883362
aF0.13487648963928223
aF0.21202310919761658
aF0.1064300388097763
aF0.1576371192932129
aF0.10365748405456543
aF0.09706003963947296
aF0.08692269027233124
aF0.08652558922767639
aF0.16510821878910065
aF0.15227793157100677
aF0.09489336609840393
aF0.05379955470561981
aF0.09634121507406235
aF0.07371117174625397
aF0.08024812489748001
aF0.34718525409698486
aF0.14562764763832092
aF0.0877726823091507
aF0.05802559107542038
aF0.07252475619316101
aF0.058224890381097794
aF0.053545404225587845
aF0.04910288751125336
aF0.16258931159973145
aF0.08961985260248184
aF0.06791521608829498
aF0.07914113998413086
aF0.019842207431793213
aF0.18602725863456726
aF0.11120156943798065
aF0.08056358993053436
aF0.15640383958816528
aF0.13341572880744934
aF0.054870449006557465
aF0.05289158970117569
aF0.09453529119491577
aF0.10093577951192856
aF0.11435611546039581
aF0.04433135688304901
aF0.041181400418281555
aF0.06488397717475891
aF0.05790891870856285
aF0.09079191088676453
aF0.07076123356819153
aF0.251238614320755
aF0.08130437880754471
aF0.09044543653726578
aF0.09195373952388763
aF0.03516010195016861
aF0.06682144850492477
aF0.0563662089407444
aF0.08952756226062775
aF0.07169768959283829
aF0.09224669635295868
aF0.13520470261573792
aF0.06017293781042099
aF0.0593213327229023
aF0.15224379301071167
aF0.06995612382888794
aF0.10234549641609192
aF0.13820423185825348
aF0.0395657941699028
aF0.039802223443984985
aF0.040791142731904984
aF0.07838721573352814
aF0.07968732714653015
aF0.14655330777168274
aF0.07231555134057999
aF0.07215671986341476
aF0.0909213200211525
aF0.06476867944002151
aF0.018186606466770172
aF0.1424119919538498
aF0.08378733694553375
aF0.07473164796829224
aF0.020178478211164474
aF0.03394453972578049
aF0.04974755272269249
aF0.05535164475440979
aF0.06830309331417084
aF0.056659046560525894
aF0.08267119526863098
aF0.06596388667821884
aF0.03485821187496185
aF0.08062431961297989
aF0.10186608135700226
aF0.0650860071182251
aF0.08318346738815308
aF0.057590607553720474
aF0.08182224631309509
aF0.05100199580192566
aF0.07898084819316864
aF0.0467408262193203
aF0.07423132658004761
aF0.0685129463672638
aF0.0532943531870842
aF0.09702549874782562
aF0.05690144747495651
aF0.04095596820116043
aF0.024181844666600227
aF0.05648673698306084
aF0.03384748846292496
aF0.12339571118354797
aF0.08579117059707642
aF0.10994915664196014
aF0.04338110610842705
aF0.03879532963037491
aF0.05265383794903755
aF0.03727731108665466
aF0.039670057594776154
aF0.014675211161375046
aF0.04683292284607887
aF0.08169934153556824
aF0.050361935049295425
aF0.0106807267293334
aF0.06049502640962601
aF0.05206695944070816
aF0.014908644370734692
aF0.016044097021222115
aF0.05923370271921158
aF0.020163290202617645
aF0.01121293380856514
aF0.03207164257764816
aF0.04182213544845581
aF0.01109482441097498
aF0.021731607615947723
aF0.01732512377202511
aF0.008512109518051147
aF0.11687885969877243
aF0.06061442568898201
aF0.05393310263752937
aF0.05243179202079773
aF0.0372299924492836
aF0.007564947009086609
aF0.009536581113934517
aF0.026963792741298676
aF0.016945740208029747
aF0.016657192260026932
aF0.026162240654230118
aF0.024813484400510788
aF0.023852646350860596
aF0.020040709525346756
aF0.005177263170480728
aF0.06643939018249512
aF0.053774312138557434
aF0.023755228146910667
aF0.005490412469953299
aF0.03520987182855606
aF0.034771814942359924
aF0.060397919267416
aF0.013642086647450924
aF0.013697718270123005
aF0.012890690006315708
aF0.09173785895109177
aF0.03560302406549454
aF0.014025009237229824
aF0.06326910853385925
aF0.016191652044653893
aF0.030434289947152138
aF0.018918082118034363
aF0.023400798439979553
aF0.07412408292293549
aF0.037588249891996384
aF0.027990998700261116
aF0.02698158100247383
aF0.037382613867521286
aF0.037955448031425476
aF0.009885089471936226
aF0.017221491783857346
aF0.007267629262059927
asVcheckpoint/checkpoint_train_loss
p43
(lp44
F2.3027536869049072
aF0.1565341353416443
aF0.08238720148801804
aF0.052677880972623825
aF0.0402660146355629
aF0.03615124523639679
asVcheckpoint/checkpoint_test_loss
p45
(lp46
F2.3027360439300537
aF0.15844202041625977
aF0.09706538170576096
aF0.07650700211524963
aF0.07066372036933899
aF0.07565215975046158
asVhyperparams/learning_rate
p47
(lp48
F0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
asVtime/convergence_iterations
p49
(lp50
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.