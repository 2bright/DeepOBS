(dp0
S'checkpoint/checkpoint_steps'
p1
(lp2
I0
aI1
aI2
aI3
aI4
aI5
asVcheckpoint/checkpoint_train_acc
p3
(lp4
F0.10828718543052673
aF0.936313271522522
aF0.974782407283783
aF0.9830893874168396
aF0.9888251423835754
aF0.9925830960273743
asVtime/percentage_convergence_performance
p5
(lp6
F0.1074562817811966
aF0.9628245830535889
aF0.9960606098175049
aF1.0023815631866455
aF1.0058479309082031
aF1.0087025165557861
asVcheckpoint/checkpoint_test_acc
p7
(lp8
F0.10423259437084198
aF0.9339398741722107
aF0.9661787748336792
aF0.9723101258277893
aF0.9756724834442139
aF0.9784414768218994
asS'training/training_steps'
p9
(lp10
I1
aI11
aI21
aI31
aI41
aI51
aI61
aI71
aI81
aI91
aI101
aI111
aI121
aI131
aI141
aI151
aI161
aI171
aI181
aI191
aI201
aI211
aI221
aI231
aI241
aI251
aI261
aI271
aI281
aI291
aI301
aI311
aI321
aI331
aI341
aI351
aI361
aI371
aI381
aI391
aI401
aI411
aI421
aI431
aI441
aI451
aI461
aI471
aI481
aI491
aI501
aI511
aI521
aI531
aI541
aI551
aI561
aI571
aI581
aI591
aI601
aI611
aI621
aI631
aI641
aI651
aI661
aI671
aI681
aI691
aI701
aI711
aI721
aI731
aI741
aI751
aI761
aI771
aI781
aI791
aI801
aI811
aI821
aI831
aI841
aI851
aI861
aI871
aI881
aI891
aI901
aI911
aI921
aI931
aI941
aI951
aI961
aI971
aI981
aI991
aI1001
aI1011
aI1021
aI1031
aI1041
aI1051
aI1061
aI1071
aI1081
aI1091
aI1101
aI1111
aI1121
aI1131
aI1141
aI1151
aI1161
aI1171
aI1181
aI1191
aI1201
aI1211
aI1221
aI1231
aI1241
aI1251
aI1261
aI1271
aI1281
aI1291
aI1301
aI1311
aI1321
aI1331
aI1341
aI1351
aI1361
aI1371
aI1381
aI1391
aI1401
aI1411
aI1421
aI1431
aI1441
aI1451
aI1461
aI1471
aI1481
aI1491
aI1501
aI1511
aI1521
aI1531
aI1541
aI1551
aI1561
aI1571
aI1581
aI1591
aI1601
aI1611
aI1621
aI1631
aI1641
aI1651
aI1661
aI1671
aI1681
aI1691
aI1701
aI1711
aI1721
aI1731
aI1741
aI1751
aI1761
aI1771
aI1781
aI1791
aI1801
aI1811
aI1821
aI1831
aI1841
aI1851
aI1861
aI1871
aI1881
aI1891
aI1901
aI1911
aI1921
aI1931
aI1941
aI1951
aI1961
aI1971
aI1981
aI1991
aI2001
aI2011
aI2021
aI2031
aI2041
aI2051
aI2061
aI2071
aI2081
aI2091
aI2101
aI2111
aI2121
aI2131
aI2141
aI2151
aI2161
aI2171
aI2181
aI2191
aI2201
aI2211
aI2221
aI2231
aI2241
aI2251
aI2261
aI2271
aI2281
aI2291
aI2301
aI2311
aI2321
aI2331
aI2341
asVhyperparams/batch_size
p11
(lp12
F128.0
aF128.0
aF128.0
aF128.0
aF128.0
aF128.0
asS'args'
p13
ccopy_reg
_reconstructor
p14
(cargparse
Namespace
p15
c__builtin__
object
p16
Ntp17
Rp18
(dp19
S'wd'
p20
NsS'data_dir'
p21
S'data_tfobs'
p22
sS'train_log_interval'
p23
I10
sS'nologs'
p24
I00
sS'num_epochs'
p25
I5
sS'lr_sched_epochs'
p26
NsS'saveto'
p27
S'res/benchmark_small_final/'
p28
sS'lr_sched_factors'
p29
NsS'run_name'
p30
S'SGD/'
p31
sS'checkpoint_epochs'
p32
I1
sS'print_train_iter'
p33
I00
sS'lr'
p34
F0.2511886
sS'bs'
p35
I128
sS'pickle'
p36
I01
sS'random_seed'
p37
I49
sS'no_time'
p38
I00
sS'test_problem'
p39
S'mnist.mnist_mlp'
p40
sbsVtraining/training_loss
p41
(lp42
F2.303356647491455
aF2.2922329902648926
aF2.261472225189209
aF2.1995091438293457
aF1.9179131984710693
aF1.7591376304626465
aF1.4246288537979126
aF1.0657693147659302
aF0.8468513488769531
aF1.201109528541565
aF0.8235350251197815
aF0.4709048867225647
aF0.4060024619102478
aF0.3857995271682739
aF0.41227987408638
aF0.3895493447780609
aF0.30691349506378174
aF0.2470390349626541
aF0.35539084672927856
aF0.18373781442642212
aF0.32860225439071655
aF0.3439668118953705
aF0.21102604269981384
aF0.4224603772163391
aF0.2850227952003479
aF0.26284024119377136
aF0.43428438901901245
aF0.370566189289093
aF0.26280760765075684
aF0.1864013373851776
aF0.28067225217819214
aF0.1965366154909134
aF0.18104632198810577
aF0.17887236177921295
aF0.28420430421829224
aF0.20426718890666962
aF0.331137090921402
aF0.13288231194019318
aF0.13280874490737915
aF0.250861793756485
aF0.20937854051589966
aF0.13053739070892334
aF0.19073538482189178
aF0.2321796715259552
aF0.3092350661754608
aF0.2790207266807556
aF0.10539018362760544
aF0.1937093436717987
aF0.11016767472028732
aF0.21068929135799408
aF0.20802642405033112
aF0.16853846609592438
aF0.11483897268772125
aF0.16233746707439423
aF0.11094572395086288
aF0.1507801115512848
aF0.17877191305160522
aF0.08123436570167542
aF0.16172939538955688
aF0.1187887191772461
aF0.13086190819740295
aF0.1583520770072937
aF0.10962879657745361
aF0.09969667345285416
aF0.15963974595069885
aF0.10097496211528778
aF0.049736857414245605
aF0.12226549535989761
aF0.08054801821708679
aF0.09409753978252411
aF0.14186224341392517
aF0.09301541745662689
aF0.09419745206832886
aF0.0746675580739975
aF0.14897651970386505
aF0.15883977711200714
aF0.12219619750976562
aF0.06722018122673035
aF0.11088037490844727
aF0.11193390190601349
aF0.08549892157316208
aF0.07608029991388321
aF0.12853360176086426
aF0.11310964822769165
aF0.21626543998718262
aF0.07464651763439178
aF0.11019720137119293
aF0.10681281983852386
aF0.14816595613956451
aF0.10159841924905777
aF0.10190193355083466
aF0.12532103061676025
aF0.15407729148864746
aF0.06484011560678482
aF0.06283354759216309
aF0.07508239150047302
aF0.1367684304714203
aF0.055966831743717194
aF0.027854541316628456
aF0.08456455171108246
aF0.05956015735864639
aF0.15751996636390686
aF0.034813426434993744
aF0.07529189437627792
aF0.12462389469146729
aF0.07514739781618118
aF0.0404554083943367
aF0.10375392436981201
aF0.07551425695419312
aF0.07472580671310425
aF0.062281955033540726
aF0.21436333656311035
aF0.06645558774471283
aF0.08592686057090759
aF0.07097770273685455
aF0.13696317374706268
aF0.06826696544885635
aF0.052925899624824524
aF0.1006755456328392
aF0.09050530940294266
aF0.10174547135829926
aF0.06461738049983978
aF0.1271851360797882
aF0.09068085998296738
aF0.14371947944164276
aF0.11407702416181564
aF0.027036389335989952
aF0.10474099218845367
aF0.0806054100394249
aF0.07672460377216339
aF0.14291509985923767
aF0.032589346170425415
aF0.07814022898674011
aF0.072761170566082
aF0.03226500004529953
aF0.21426013112068176
aF0.08283982425928116
aF0.07731085270643234
aF0.0764518529176712
aF0.13616326451301575
aF0.04437682777643204
aF0.06184474378824234
aF0.07395580410957336
aF0.025283705443143845
aF0.041340406984090805
aF0.013590292073786259
aF0.0112305898219347
aF0.024725064635276794
aF0.08358226716518402
aF0.07224734127521515
aF0.03555469214916229
aF0.11445892602205276
aF0.09380410611629486
aF0.07513904571533203
aF0.10114066302776337
aF0.06619608402252197
aF0.06736588478088379
aF0.03406517952680588
aF0.04670468717813492
aF0.06427215784788132
aF0.058029819279909134
aF0.0636020079255104
aF0.09452402591705322
aF0.06600242853164673
aF0.03642713278532028
aF0.06885090470314026
aF0.039491139352321625
aF0.05250414460897446
aF0.020081670954823494
aF0.03573739528656006
aF0.03336668759584427
aF0.03376070782542229
aF0.02162754349410534
aF0.03776121512055397
aF0.02953711524605751
aF0.06299179047346115
aF0.023606644943356514
aF0.025107497349381447
aF0.07632029056549072
aF0.0673108622431755
aF0.04428250715136528
aF0.04860909283161163
aF0.026511970907449722
aF0.027273520827293396
aF0.015147984027862549
aF0.03802409768104553
aF0.020976144820451736
aF0.06175900623202324
aF0.021924424916505814
aF0.05000143125653267
aF0.027078425511717796
aF0.03972155600786209
aF0.044575534760951996
aF0.011944913305342197
aF0.024804485961794853
aF0.05526597052812576
aF0.017465464770793915
aF0.05637664347887039
aF0.010846851393580437
aF0.022248823195695877
aF0.07979215681552887
aF0.0529906190931797
aF0.012513353489339352
aF0.055055104196071625
aF0.03283555805683136
aF0.05678023397922516
aF0.04974311590194702
aF0.016228623688220978
aF0.029957029968500137
aF0.06376491487026215
aF0.03912365064024925
aF0.022020570933818817
aF0.0428830124437809
aF0.06488985568284988
aF0.007830959744751453
aF0.018084688112139702
aF0.06576918065547943
aF0.033472321927547455
aF0.009963047690689564
aF0.03317883238196373
aF0.032066576182842255
aF0.013229873962700367
aF0.008556198328733444
aF0.07362468540668488
aF0.01792517676949501
aF0.029607437551021576
aF0.03212464600801468
aF0.011224200949072838
aF0.012656128965318203
aF0.026519542559981346
aF0.0100371353328228
aF0.02348378300666809
aF0.02620912529528141
aF0.07174122333526611
aF0.02351340651512146
asVcheckpoint/checkpoint_train_loss
p43
(lp44
F2.3028128147125244
aF0.20591077208518982
aF0.08713460713624954
aF0.054495058953762054
aF0.038609299808740616
aF0.02549508586525917
asVcheckpoint/checkpoint_test_loss
p45
(lp46
F2.3028457164764404
aF0.21005241572856903
aF0.10731394588947296
aF0.08499027043581009
aF0.07729556411504745
aF0.06878697127103806
asVhyperparams/learning_rate
p47
(lp48
F0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
aF0.25118860602378845
asVtime/convergence_iterations
p49
(lp50
F0.0
aF0.0
aF0.0
aF3.0
aF3.0
aF3.0
as.